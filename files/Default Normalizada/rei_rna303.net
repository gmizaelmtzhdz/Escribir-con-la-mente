FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=9 11 6 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (0, 6, 0.00000000000000000000e+000) (11, 6, 5.00000000000000000000e-001) (11, 6, 5.00000000000000000000e-001) (11, 6, 5.00000000000000000000e-001) (11, 6, 5.00000000000000000000e-001) (11, 6, 5.00000000000000000000e-001) (0, 6, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.58949665654336210000e+000) (1, -6.01273826355126850000e+000) (2, 5.85691174712652800000e+000) (3, -2.94223891316413880000e+001) (4, 1.10062044114758390000e+001) (5, 6.23559166698856300000e+001) (6, 9.06378759926145610000e+000) (7, -1.15333706603295380000e+001) (8, 1.30307982079876150000e+000) (0, -6.16789997431248870000e+000) (1, -2.22526577338197380000e+001) (2, 7.53540963822310950000e+000) (3, -1.65545813635854730000e+001) (4, -1.05713801041771430000e+001) (5, 1.58443951560936500000e+001) (6, -1.48839859122013390000e+001) (7, 1.68375748068049980000e+001) (8, 7.25574049737479740000e-001) (0, 3.06414474043956000000e-001) (1, 1.21410560592691820000e+001) (2, -5.78219241620663290000e+000) (3, 1.72348225856380970000e+000) (4, 8.60643999889619040000e-001) (5, -9.02662280926374190000e+000) (6, 2.34439848408007960000e+000) (7, 5.41942898932695760000e-002) (8, 8.27168498984487630000e-001) (0, 2.24606723737585770000e+000) (1, 1.17826082159394660000e+002) (2, 4.64784663266616250000e+001) (3, -2.51067594080835330000e+002) (4, -1.44944852069784730000e+002) (5, 2.20512231556907320000e+002) (6, 1.05485924250936950000e+002) (7, 1.13859071659044360000e+000) (8, 3.01701796261432790000e+000) (0, 8.35509153605644710000e+000) (1, 2.51824195561341670000e+001) (2, 1.16999419653074010000e+000) (3, 1.23371295177645730000e+001) (4, 2.14156233018347700000e+000) (5, -2.68872351884166070000e+001) (6, 1.67116684447675890000e+001) (7, -4.02686123640523380000e+001) (8, 7.69441344026063230000e-001) (0, 5.24672291947006820000e-001) (1, -5.01732693021101370000e+000) (2, 3.43921889192374540000e+000) (3, 4.69147973868373040000e+000) (4, 1.01023092434618710000e+000) (5, 2.08159449503068430000e+001) (6, 6.86427098737050210000e-001) (7, -4.55815615659214140000e+000) (8, 7.31352738071259670000e-001) (0, 1.41277134595525710000e+000) (1, 1.16244287145357350000e+001) (2, -9.40650596173872880000e+000) (3, 9.19763698372930700000e+000) (4, -8.56698133206334480000e-001) (5, -8.51416244468135730000e+000) (6, 3.46397873190219620000e+000) (7, 1.43858543957740020000e+001) (8, 3.95423810497223690000e-001) (0, 2.43342596765059560000e-002) (1, 2.38427381915436300000e-001) (2, -5.01423264619619720000e-001) (3, 7.97693424388887350000e-001) (4, 4.73302868109189200000e-001) (5, -1.58520410757749920000e+000) (6, 5.56158916249033350000e-001) (7, -2.92603459141944190000e-001) (8, 2.16061214478491610000e-001) (0, 8.24338361127319890000e+000) (1, 4.03714822965309490000e+000) (2, -6.96613022416796750000e+000) (3, 1.16469329789996440000e+001) (4, 2.50656500919941930000e+001) (5, -2.75272309373491490000e+001) (6, 1.13858544659796440000e+001) (7, -1.30228097421432110000e+001) (8, 6.97838233354330930000e-001) (0, 2.35141936618135370000e-001) (1, -1.13859190887641870000e+000) (2, 7.33854601636496410000e-001) (3, -8.93869313160392090000e-001) (4, 3.18651652704021030000e+000) (5, -3.44821294033629130000e+000) (6, 2.34535258656163670000e+000) (7, -5.85402042509999720000e+000) (8, 2.28820829919812540000e-001) (9, -6.35530927633094040000e-001) (10, -7.88069314053928220000e-001) (11, -4.07482984132125080000e-002) (12, 1.90477959778907190000e-001) (13, -7.19542156356576260000e-001) (14, 4.40921019921899530000e-001) (15, -9.28245182847791560000e-001) (16, 6.77845399167563570000e+000) (17, -1.58064067041345150000e-001) (18, 7.58665524191092040000e-003) (19, 7.13628961445279100000e-001) (9, 2.17677408946646670000e-001) (10, 5.76653837234744280000e-001) (11, 1.03300540960867560000e+000) (12, -1.40415283428509900000e-001) (13, 2.46087414179564030000e-001) (14, 3.04202637170831620000e-001) (15, 5.47262670131351880000e-001) (16, -8.49109666065348100000e+000) (17, -8.65726907517719440000e-002) (18, 9.98130261730795440000e-001) (19, 7.25569367662744050000e-001) (9, 7.03120020067646620000e-001) (10, 4.23364076151404420000e-001) (11, 8.22421532317552970000e-003) (12, -1.18544665255299230000e-001) (13, 6.89690172715356260000e-001) (14, -1.95913805645179990000e-001) (15, 3.50486188246921170000e-002) (16, 4.99352068469475970000e+000) (17, -4.37560091872943310000e-001) (18, -1.46206554884774740000e+000) (19, 1.91067102168371390000e-001) (9, -7.81717126405255590000e-001) (10, 1.12536804708490100000e+000) (11, 8.93198263193755610000e-001) (12, 2.17701760083824010000e-002) (13, 1.83424317408492220000e-001) (14, 1.57295068875550910000e+000) (15, 1.41708143791591910000e+000) (16, -1.55399832640904980000e+001) (17, 8.19990809005417990000e-001) (18, 2.90180135839588440000e+000) (19, 7.91939923546028160000e-001) (9, 2.89679854490737910000e-001) (10, 8.31631630033513260000e-001) (11, 7.37925014781484420000e-001) (12, -1.84514182056954750000e-001) (13, 2.74776213027389300000e-001) (14, 1.17057231987329330000e-001) (15, 1.12906380352047210000e-001) (16, -5.68201116766326830000e+000) (17, 9.17498563057687510000e-001) (18, -7.61184970736788590000e-001) (19, 6.88759411014291720000e-001) 
