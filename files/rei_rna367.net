FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=9 11 6 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (0, 6, 0.00000000000000000000e+000) (11, 6, 5.00000000000000000000e-001) (11, 6, 5.00000000000000000000e-001) (11, 6, 5.00000000000000000000e-001) (11, 6, 5.00000000000000000000e-001) (11, 6, 5.00000000000000000000e-001) (0, 6, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -1.98409690789096740000e+000) (1, -9.67169652642855840000e+000) (2, -1.13026799342737690000e+001) (3, -2.56740911986447530000e+001) (4, 4.59772170873180120000e+000) (5, 3.31002687703057530000e+001) (6, -1.96635142433918200000e+001) (7, 3.96936721364073150000e+001) (8, -1.24590773919156720000e+000) (0, 8.31582249524318230000e+001) (1, 3.01257256647457100000e+002) (2, -2.87934073142033130000e+002) (3, 1.58050545475370600000e+002) (4, 1.50000000000000000000e+003) (5, -9.76033734962353830000e+002) (6, 5.12970250298435420000e+002) (7, 2.07692772669928220000e+002) (8, -2.11279027244616910000e+001) (0, -2.49918064150586570000e+000) (1, 1.32247321108916170000e+001) (2, -4.20834160769818170000e-001) (3, -3.10943961192378680000e+000) (4, -8.88882341105002990000e+000) (5, -6.48986691431299610000e+000) (6, 2.50603388200931400000e+001) (7, -2.01716086966223290000e+001) (8, 8.31742535865710230000e-001) (0, 1.15166313379800480000e+000) (1, 5.38965970414469100000e+001) (2, -4.04341992215067450000e+001) (3, -1.27173487339838420000e+002) (4, 1.36501328440914160000e+002) (5, 4.59186725798564340000e+001) (6, 4.76570155955494950000e+001) (7, -1.61217245834229780000e+001) (8, -2.54650248864212300000e+000) (0, 5.42684035197041560000e-001) (1, 7.39195562850413260000e+000) (2, -1.60957832996935310000e+000) (3, -5.55723946706964080000e+000) (4, 1.02194715681004510000e+001) (5, -7.18423448832676130000e+000) (6, 6.23390250536265800000e+000) (7, -1.19901055302695770000e+001) (8, -7.72814150589186900000e-001) (0, -1.26483124334069220000e-002) (1, -1.42088937670835060000e+000) (2, 1.17366889902599690000e+000) (3, 8.83020392905706890000e-001) (4, -4.11597135193503670000e+000) (5, 4.87813142640148990000e-002) (6, 1.34812019960082230000e+000) (7, 1.49380189291572400000e+000) (8, -1.22159184938018250000e+000) (0, -1.05236553155123500000e+002) (1, 2.33205717194313190000e+002) (2, -1.84117557897643710000e+002) (3, 4.03741964535514280000e+002) (4, -1.27041650614850580000e+002) (5, 1.02046820792763400000e+001) (6, 5.25144674678774780000e+002) (7, -5.40710867668043080000e+002) (8, 3.21916625484028810000e+001) (0, 5.01062592344321000000e+000) (1, -7.29440240491589260000e+000) (2, 1.24843409315875680000e+001) (3, 9.89306195905128940000e+000) (4, -6.79882210463515510000e+000) (5, -1.55356501713478310000e+001) (6, 2.39973808495764420000e+000) (7, -6.57207429137540090000e+000) (8, -1.45634278154067930000e+000) (0, 4.03923467713064180000e+000) (1, 3.55869583277840150000e+000) (2, 6.35325944558925840000e+000) (3, 1.91872808690878350000e+000) (4, 9.53842743213059660000e+000) (5, -2.34822357368410660000e+001) (6, 7.96969061716363920000e+000) (7, -1.51296578713778250000e+001) (8, -1.09761573385013000000e+000) (0, -6.74444067349668900000e+001) (1, -3.03857314764959310000e+002) (2, -2.66669121331745100000e+001) (3, 8.16186635504880540000e+002) (4, 1.50000000000000000000e+003) (5, -3.45568589448851240000e+002) (6, 4.78930631677241650000e+002) (7, -1.38186813726047970000e+003) (8, 3.90210558915228130000e+001) (9, 3.62856277394659870000e-001) (10, -3.92423575778653830000e-003) (11, 3.42716514086256860000e-001) (12, -1.75520831497746470000e-002) (13, -1.42665950716827390000e+000) (14, 5.43567181239827180000e-002) (15, -6.65359228984556680000e-002) (16, -9.36091750457352820000e-001) (17, 1.46778909440493300000e+000) (18, 2.61842826440426670000e-001) (19, 1.29700353533321070000e-001) (9, -8.43739296742932740000e-002) (10, -1.19231860841274100000e-002) (11, -2.56250367079636610000e-001) (12, -2.07209012489506070000e-002) (13, 1.59612088936898470000e+000) (14, 2.93756700495482190000e-001) (15, 1.16866220543430200000e-001) (16, 8.26045564889519860000e-001) (17, -1.40111062044003210000e+000) (18, -3.66655495949450680000e-001) (19, 1.64835781586285070000e+000) (9, -4.48822583096284920000e-001) (10, -7.61324257363971970000e-002) (11, -5.00775689928076860000e-001) (12, 8.68219478269283140000e-002) (13, 7.26500817206059750000e-001) (14, 3.92276967082859420000e+000) (15, 2.23067215176163900000e-001) (16, -8.65126318703654390000e-002) (17, -3.58332727746669240000e-001) (18, -1.88184438935227350000e-001) (19, 3.02498766325863010000e+000) (9, 7.50953625336719390000e-001) (10, 1.55451686484221390000e-001) (11, -2.99543489112422010000e-001) (12, -3.46239739845483670000e-001) (13, 4.19221248142048260000e+000) (14, 1.48205689613621330000e+000) (15, 4.31022969672833540000e-001) (16, 2.55630568727594950000e+000) (17, -2.81938948413717670000e+000) (18, -9.39197452110454860000e-002) (19, 3.48096386652630270000e+000) (9, -4.78179755373533850000e-001) (10, 5.18229725271765820000e-003) (11, -1.22875951185331010000e+000) (12, 1.57941827100969670000e-004) (13, 1.76113135532741930000e+000) (14, 7.42702258299864230000e+000) (15, 2.39202066674194970000e-001) (16, -2.74628908667151170000e-001) (17, -5.11060775229525420000e-001) (18, -1.45684593365909720000e-001) (19, 5.32251173634996630000e+000) 
