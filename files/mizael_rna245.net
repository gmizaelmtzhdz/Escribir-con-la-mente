FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999977796e-01
cascade_candidate_limit=1.00000000000000000000e+03
cascade_weight_multiplier=4.00000000000000022204e-01
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-01 5.00000000000000000000e-01 7.50000000000000000000e-01 1.00000000000000000000e+00 
layer_sizes=9 11 6 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (0, 0, 0.00000000000000000000e+00) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (9, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (11, 6, 5.00000000000000000000e-01) (0, 6, 0.00000000000000000000e+00) 
connections (connected_to_neuron, weight)=(0, 3.94239369034767861422e-02) (1, 2.01314920485019754892e-01) (2, 1.66428977549076151377e-01) (3, 8.93757212162018532808e-02) (4, 1.48816654384136271005e-01) (5, 4.17470055818558449801e-02) (6, 2.83797997236252541597e-02) (7, 5.59815037250519509371e-02) (8, 1.71179437935352396494e-01) (0, 2.69530582427979226168e-02) (1, 6.70208539068699593599e-02) (2, 1.79453441798687052255e-01) (3, 2.11833903491497110849e-01) (4, 1.79642090499401163584e-01) (5, 8.22203959524632210787e-02) (6, 1.52754465043544840341e-01) (7, 2.13184276521205973154e-01) (8, 6.92438910901547188814e-02) (0, 1.13403344452381205087e-01) (1, 9.52702182531357522066e-02) (2, 1.34836660623550486093e-01) (3, 3.28653398156166787203e-02) (4, 1.77367458045482706552e-01) (5, 1.84385994374752115732e-01) (6, 1.67486512959003519541e-01) (7, 1.38535225689411234384e-01) (8, 1.33233332931995462900e-01) (0, 2.04840326607227396494e-01) (1, 2.21240165829659218844e-02) (2, 1.04231120944023203378e-01) (3, 1.83677101433277201181e-01) (4, 4.15479558706284279879e-02) (5, 8.55460259318352456148e-02) (6, 1.30106070935726236826e-01) (7, 1.10923664569854807382e-01) (8, 2.14362675249576639658e-01) (0, 1.51853064000606607920e-01) (1, 1.19303451776504587656e-01) (2, 5.03441783785820717867e-02) (3, 1.03032493889331888681e-01) (4, 1.26256512403488230234e-01) (5, 9.73650234937668557222e-02) (6, 6.24859239161015267428e-02) (7, 1.18090400397777628427e-01) (8, 5.70071059465409035738e-02) (0, -1.15293705761432718759e-01) (1, -1.89155148267746042734e-01) (2, -1.89808638691902231699e-01) (3, -6.60498085618019814547e-02) (4, -9.57518088817597146090e-02) (5, -1.14538432955741953378e-01) (6, -1.51213155984878611093e-01) (7, -8.28864666819573159273e-02) (8, -1.57170982956886362558e-01) (0, 5.31728434562683816011e-02) (1, 1.04600051343441080576e-01) (2, 2.01364258229732584482e-01) (3, 1.66406163871288370615e-01) (4, 8.94403773546219582613e-02) (5, 2.03488269746303629404e-01) (6, 5.06372842192650551851e-02) (7, 5.31174707412720437105e-02) (8, 2.50362101197243447359e-02) (0, 1.16183305084705423837e-01) (1, 1.63223544061184000498e-01) (2, 1.15959869623184275156e-01) (3, 1.10545972287654947763e-01) (4, 9.50766000151634926851e-02) (5, 2.15263316333293985849e-01) (6, 1.40890138149261545664e-01) (7, 1.78109088838100504404e-01) (8, 1.21519828140735697275e-01) (0, 2.18255156576633524423e-01) (1, 2.05950084328652138765e-02) (2, 2.19610223472118448740e-01) (3, 5.52622544765473122652e-02) (4, 1.25301318168640207773e-01) (5, 5.04550802707672829683e-02) (6, 8.54536163806915993746e-02) (7, 7.92514997720719094332e-02) (8, 1.74703279435634684091e-01) (0, 1.90915191471576761728e-01) (1, 1.48038336932659220224e-01) (2, 1.11816810369491648203e-01) (3, 5.37442135810852761324e-02) (4, 1.81211182773113321787e-01) (5, 1.96416864097118448740e-01) (6, 3.51084637641907448824e-02) (7, 1.27617346048355173593e-01) (8, 6.58572222292423958834e-02) (9, 2.18596720993518900400e-01) (10, -8.17453804612160439547e-02) (11, 9.89746916294098610933e-02) (12, 2.36329305171967263277e-02) (13, -1.85562090873718332773e-01) (14, -1.97801785469055246836e-01) (15, -1.20407218039035868173e-01) (16, 1.44983889460563730722e-01) (17, 1.17274822592735361582e-01) (18, -1.25143902301788401132e-01) (19, 6.58740158379078621920e-02) (9, 7.53839071094990487154e-02) (10, -2.36240866780281777437e-02) (11, 6.41291643679142708834e-02) (12, 7.59789067506790871676e-02) (13, -2.40138712525368447359e-02) (14, -1.40608595609664988046e-01) (15, -5.87198081612587685640e-02) (16, 4.64412066340447182711e-02) (17, 1.64845028817653727060e-01) (18, -1.99468316435813974863e-01) (19, 2.01144481003284525400e-01) (9, 1.35760227143764566904e-01) (10, -7.14299771189690346773e-02) (11, 9.29612758755684609469e-02) (12, 1.69504443109035562998e-01) (13, -1.10218802392482828623e-01) (14, -1.70621884912252497202e-01) (15, -5.53871187567711587008e-02) (16, 3.73985487222672219332e-02) (17, 1.15235345363617014414e-01) (18, -5.67904058098793740328e-02) (19, 1.75653168857097696787e-01) (9, 1.94210031926631998545e-01) (10, -5.31574729084969277437e-02) (11, 2.10091078579425882822e-01) (12, 2.16408261954784464365e-01) (13, -1.53564678430557322031e-01) (14, -1.04925060570240091806e-01) (15, -1.26316944062709879404e-01) (16, 1.81291440427303385263e-01) (17, 1.80948967039585184580e-01) (18, -7.09330382943154091890e-02) (19, 1.77667358815670084482e-01) (9, 2.50781270861626381929e-02) (10, -2.14954139590263437753e-01) (11, 1.73653492629528116709e-01) (12, 1.04469539523124765878e-01) (13, -5.36739322543144936617e-02) (14, -3.99053189158440346773e-02) (15, -1.90685452818870615488e-01) (16, 2.06857765018940042978e-01) (17, 1.81239167153835367685e-01) (18, -7.49252381920815224703e-02) (19, 1.55427807867527079111e-01) 
