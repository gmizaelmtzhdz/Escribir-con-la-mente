FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=9 4 6 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (0, 6, 0.00000000000000000000e+000) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (0, 6, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 5.16689689214042860000e+000) (1, -7.73716556447208780000e+000) (2, -2.57116495844662830000e+000) (3, -1.32604448849911850000e+000) (4, 1.21211044225948540000e+001) (5, -9.17896268553523060000e+000) (6, -1.71922302980205470000e+001) (7, 8.88269044939321260000e+000) (8, -1.66798694674346710000e-001) (0, 7.65223545964157200000e+000) (1, -2.64697349482827620000e+000) (2, 2.48980846614352560000e+000) (3, -4.01230876466414180000e+000) (4, 2.03460625908743790000e+001) (5, -3.01258752735003090000e+001) (6, -3.89258904015480400000e+000) (7, 7.94705869915434680000e-001) (8, -1.25335024089381310000e+000) (0, -3.59797108797672630000e-001) (1, -6.42173802163653610000e-001) (2, -4.71161023498377960000e-001) (3, 4.59004751783062160000e-001) (4, -6.99489853588465600000e-001) (5, 1.23822847335265780000e+000) (6, -1.37434782092216020000e+000) (7, 1.31079137049383340000e+000) (8, 1.62585425653512140000e-001) (9, -4.65820781479806710000e-001) (10, 4.01398164801769490000e-001) (11, 2.21910938109830710000e+000) (12, 5.76327290318718540000e-001) (9, 6.59730342245526070000e-001) (10, -8.17691829566580930000e-001) (11, -3.77200142426714930000e+000) (12, 7.48790632887756940000e-001) (9, -5.82386808518678340000e-002) (10, -6.74624949755482650000e-002) (11, 5.34511044284362470000e-001) (12, 9.11249549067429050000e-001) (9, 9.24457093655684960000e-001) (10, -9.35031816773241880000e-001) (11, -5.99688584732452320000e+000) (12, 1.05494221572823600000e+000) (9, 2.56384766852791880000e-001) (10, -9.49065776175334890000e-002) (11, 7.55661002027234320000e-002) (12, 1.05356874199884400000e+000) 
