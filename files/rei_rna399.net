FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=9 4 6 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (0, 6, 0.00000000000000000000e+000) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (0, 6, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 3.53108806696448860000e+000) (1, -1.65687658333973810000e+000) (2, 1.37257030053216100000e+000) (3, -5.28669870129114820000e-001) (4, 8.37453512034804070000e+000) (5, -1.18500426643537470000e+001) (6, 1.44048590728752510000e+000) (7, -1.23582050639100770000e+000) (8, -6.06316173033815020000e-001) (0, -1.80806976010434250000e+000) (1, 6.55547060970286590000e-001) (2, -7.72163411693773890000e-001) (3, 5.58086030896431830000e-001) (4, -3.89196548863236020000e+000) (5, 5.10502484173562810000e+000) (6, -7.00057293148733770000e-001) (7, 1.25790893475375490000e+000) (8, 4.34558890452471920000e-001) (0, -4.63318728413094940000e+000) (1, 1.77900976262891540000e+000) (2, -2.82278201374037430000e+000) (3, 4.79006776925631480000e-001) (4, -1.23336165062985150000e+001) (5, 2.13608595804440460000e+001) (6, -4.90026425273542900000e+000) (7, 3.10366299226583210000e+000) (8, 1.25415398210642140000e+000) (9, -8.48996572478828430000e-001) (10, 1.39067683418833040000e+000) (11, -1.21596777973510120000e+000) (12, 7.22562634501437980000e-001) (9, -1.43890907403258890000e+000) (10, -3.12499253889622340000e+000) (11, 3.74413884080292520000e-001) (12, 8.64091159164810610000e-001) (9, -1.83945321430771610000e-001) (10, 3.55496119623715690000e-001) (11, -1.01064872568622120000e-001) (12, 9.28599732533908040000e-001) (9, -5.15557923056558340000e-001) (10, -3.68016634669990820000e+000) (11, 1.04093407097896230000e+000) (12, 1.04557196364718830000e+000) (9, 3.23763827818536190000e+000) (10, 2.46325804563444790000e+000) (11, 1.44936085712457260000e+000) (12, 6.98860845804521570000e-001) 
