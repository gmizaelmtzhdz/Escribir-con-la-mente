FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=9 4 6 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (0, 6, 0.00000000000000000000e+000) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (0, 6, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -4.08834792196019330000e+000) (1, -9.25904195314356640000e-001) (2, -3.96966478329307830000e+000) (3, 3.96028653234131320000e+000) (4, -9.77899181163876820000e+000) (5, 2.30329327833008190000e+001) (6, -1.14418130237135640000e+001) (7, 1.38328279224360260000e+001) (8, 7.80138462354345360000e-001) (0, 2.04379446246419550000e+000) (1, 6.10147342399809170000e-001) (2, 1.96376200627809270000e+000) (3, -2.79025711072003760000e+000) (4, 4.42523679045283820000e+000) (5, -9.15829742724937650000e+000) (6, 5.13145441991986130000e+000) (7, -8.04246846000128100000e+000) (8, -7.56164465700310370000e-001) (0, 1.01463542761283470000e+001) (1, -8.78362364671849250000e+000) (2, -2.40530850479015700000e+000) (3, 1.57827441326517550000e+001) (4, 1.38934379354729530000e+001) (5, -2.81109927796378860000e+001) (6, -1.72601765364676350000e+000) (7, 3.20995244901214530000e+001) (8, 2.32015353464829040000e+000) (9, -1.34783559426991520000e+000) (10, -2.32322703887109540000e+000) (11, -7.00222229791404360000e-001) (12, 8.35887840284364470000e-001) (9, 1.31504357910274570000e+000) (10, 2.17063912976266240000e+000) (11, -1.40677494907169440000e-001) (12, 1.24370012835345280000e+000) (9, -2.47150398571851120000e-001) (10, -5.52753809753109620000e-001) (11, -6.33470395399673760000e-001) (12, 1.41648259591227350000e+000) (9, 2.25864814610221700000e+000) (10, 4.06833328970601740000e+000) (11, 1.00203025811993270000e+000) (12, 7.94042524978241790000e-001) (9, 2.29482935595762130000e-001) (10, 3.20429602748735780000e-001) (11, 3.34003497671318870000e-001) (12, 8.20908978154915920000e-001) 
