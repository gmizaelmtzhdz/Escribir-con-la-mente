FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=9 11 6 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (0, 6, 0.00000000000000000000e+000) (11, 6, 5.00000000000000000000e-001) (11, 6, 5.00000000000000000000e-001) (11, 6, 5.00000000000000000000e-001) (11, 6, 5.00000000000000000000e-001) (11, 6, 5.00000000000000000000e-001) (0, 6, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, -3.02064559570489970000e+000) (1, -6.68114113700461590000e+000) (2, 2.13483808320172810000e+001) (3, -2.94072515304217030000e+001) (4, 3.93404353889744910000e+001) (5, 4.06107703660673490000e+001) (6, -4.75671107596485230000e+000) (7, 2.85008549439443290000e+000) (8, 9.63414274174547500000e-001) (0, -4.76113102586682050000e+001) (1, 5.49676758868679940000e+002) (2, 7.85333034986469020000e+001) (3, -4.67052412089096950000e+002) (4, -1.50000000000000000000e+003) (5, 1.12772103855074410000e+002) (6, -4.63133090891474520000e+002) (7, 1.45713326264234390000e+003) (8, -5.83219325259753990000e+001) (0, -7.73111747753885580000e+000) (1, -1.96031402378839450000e+000) (2, -2.01760002765478850000e+001) (3, -8.93353021726558440000e+000) (4, -7.10099102345338910000e+000) (5, 3.38240935097546110000e+001) (6, -3.71134989163587580000e+001) (7, 6.29955263815321520000e+001) (8, -1.07279177131928290000e+000) (0, -5.27364655651431310000e+000) (1, -4.09685507323282930000e+000) (2, 3.69006123975619090000e+000) (3, -1.35985994698275190000e+001) (4, -1.48607473781031330000e+001) (5, 2.77696737000110440000e+001) (6, -2.63314937872006420000e+001) (7, 1.73976890141805820000e+001) (8, -7.65958397496332320000e-001) (0, 2.81012070634942890000e+001) (1, -1.37627732370730680000e+001) (2, 2.62483358205426320000e+001) (3, 3.61154448994365610000e+001) (4, 3.61514068628545860000e+001) (5, -9.61835648558639060000e+001) (6, 7.97461121631079100000e+001) (7, -1.09339711265719800000e+002) (8, 1.64710390590371270000e+000) (0, 2.19063889679129530000e+001) (1, -1.21992232013348620000e+001) (2, 1.93282374215706590000e+001) (3, 4.63378344659611660000e+001) (4, -5.68716095774639360000e+001) (5, -5.24558297162197180000e+001) (6, 1.04669474195042760000e+001) (7, -4.92086578346543040000e+001) (8, -3.55891774624521730000e+000) (0, -1.88235254333385370000e-001) (1, 1.37060733293030430000e+001) (2, -4.47514159269472530000e+001) (3, 3.36123982831116520000e+001) (4, -1.84868431137160940000e+000) (5, -2.76567599889094640000e+001) (6, 9.70338598192461180000e+000) (7, 1.63307004911266240000e+001) (8, 1.28247669851856340000e-001) (0, -4.69230567502616100000e-001) (1, -6.99667814174605020000e-002) (2, 1.43478225862352770000e+000) (3, 1.96724522276080550000e+000) (4, 1.62467568860684140000e+000) (5, -3.41071669434463500000e+000) (6, 5.63403532337991740000e+000) (7, -3.98478008803533790000e+000) (8, 1.15264815948100590000e-001) (0, -3.29872273178445860000e-001) (1, 2.12583712963574230000e+000) (2, 3.99950014320571810000e-001) (3, 7.57868536017544000000e-001) (4, 3.66538791448926430000e+000) (5, -6.19608859940124290000e+000) (6, 7.57997187053311360000e+000) (7, -3.11891677625774700000e+000) (8, -8.39600285474517040000e-001) (0, -8.46016748923476230000e-001) (1, -8.38583679612098540000e+000) (2, 1.67644214983394730000e+001) (3, -4.88645172493542110000e+000) (4, -1.66809689527248840000e+000) (5, 1.93995128928312610000e+001) (6, -5.62896889453537860000e+000) (7, -1.49202196743651780000e+001) (8, 2.09518006794994660000e+000) (9, -2.69199762618245350000e-001) (10, -2.46579808277918870000e-001) (11, -1.78816507572915700000e-001) (12, -4.87019018333875560000e-001) (13, -3.80266764259357070000e-001) (14, -1.83669533743415210000e-001) (15, -3.72515638192736030000e-001) (16, 2.45756597981338180000e+000) (17, -1.74962318092563400000e+000) (18, -1.22085956294678220000e+000) (19, 3.77104411300359990000e-001) (9, 1.63726291910943570000e-001) (10, 3.28392578317596880000e-001) (11, -6.01278976887051550000e-002) (12, 6.99064198542282520000e-001) (13, 1.10421291597207540000e-001) (14, 7.48512468838745950000e-002) (15, 3.62362862691179900000e-001) (16, -2.65111579813058860000e+000) (17, 2.29120112380062090000e+000) (18, 1.26227801192305590000e+000) (19, 1.42571520680640700000e+000) (9, 6.86785694707470970000e-001) (10, 6.14414967017183340000e-001) (11, -4.08532730373982860000e-001) (12, 1.42477264103601200000e+000) (13, 2.75425064298046580000e-001) (14, 2.35383731535247000000e-001) (15, 5.14249961759952410000e-001) (16, 6.23819011715469270000e-001) (17, -1.26342567826408680000e-001) (18, 1.35897730945357040000e-001) (19, 1.46514217271021140000e+000) (9, -2.59708151415783750000e-002) (10, -2.15314402174134110000e-001) (11, 1.40426137551639950000e+000) (12, 1.36462286596053970000e+000) (13, 1.16316009474549700000e+000) (14, 6.04557856991564170000e-001) (15, 4.65153060199882340000e-001) (16, -5.47288401535602360000e+000) (17, 6.49658855575874930000e+000) (18, 2.73746080304427820000e+000) (19, 2.61019084465720170000e+000) (9, 4.91707519205153490000e-001) (10, 2.00132081521072900000e-001) (11, 4.27344131418209090000e-001) (12, 7.40400712030339640000e-001) (13, 6.82838224751449060000e-001) (14, 2.49891716413352730000e-001) (15, 3.45449480725678440000e-001) (16, -1.62923822420969500000e+000) (17, 1.32536478266301330000e+000) (18, 6.00228890232336940000e-001) (19, 1.43838678700990470000e+000) 
