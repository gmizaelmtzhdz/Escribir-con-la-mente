FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=9 4 6 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (0, 6, 0.00000000000000000000e+000) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (0, 6, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.30940238878849140000e+000) (1, 9.88666092822620190000e-001) (2, 1.61135842710308630000e+000) (3, -1.46053301055809910000e+000) (4, 1.86491065654036550000e+000) (5, -1.77689358398882270000e+000) (6, 2.83932084706769580000e+000) (7, -6.48468742815876450000e+000) (8, -5.95544201040586810000e-001) (0, -4.68130825097124960000e+000) (1, 2.52728175152347130000e+000) (2, -2.81405139728949340000e+000) (3, 6.84735965723329180000e-001) (4, -1.41863540331810340000e+001) (5, 2.48407751172496350000e+001) (6, -5.50148805482422070000e-001) (7, 9.01912071408272680000e-001) (8, 9.87321832419855160000e-001) (0, 3.23837269109382710000e+000) (1, -7.93603882308478690000e+000) (2, -2.44316673485206690000e+000) (3, 2.75760256886133480000e+000) (4, 1.49383566084426020000e+001) (5, -2.43580817234439980000e+001) (6, -1.16819881692164230000e+001) (7, 2.19985407267111020000e+001) (8, 5.02120582014246000000e-003) (9, -1.70867640468613910000e+000) (10, -8.66345090833702920000e-001) (11, -5.90379388648446750000e-001) (12, 4.96631680586387130000e-001) (9, 2.13746001283670010000e+000) (10, 1.32319960790181220000e+000) (11, 7.36726871054578240000e-001) (12, 8.29949128045313510000e-001) (9, -3.61298481201774920000e-002) (10, 1.63666244833484270000e-001) (11, -2.15225899927222850000e-002) (12, 9.05081302021032430000e-001) (9, 3.65877576453618670000e+000) (10, 1.80172295228319210000e+000) (11, 1.20800540573023140000e+000) (12, 1.21337644642842710000e+000) (9, 8.61516370232966680000e-001) (10, 5.64934240443703880000e-001) (11, 5.62119268069280810000e-001) (12, 1.07164660289696490000e+000) 
