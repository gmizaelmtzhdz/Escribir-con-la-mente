FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=9 4 6 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (9, 6, 5.00000000000000000000e-001) (0, 6, 0.00000000000000000000e+000) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (4, 6, 5.00000000000000000000e-001) (0, 6, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 5.59601055451660060000e-001) (1, -6.48691271373027290000e-001) (2, 1.80247570214841110000e-001) (3, -2.03437247198371640000e-001) (4, 1.27709951616629920000e+000) (5, -8.55247238126826680000e-001) (6, -4.89895618201217520000e-001) (7, -3.72043022827733770000e-001) (8, -2.50377186119041390000e-002) (0, -2.54019305285872580000e+000) (1, 2.93446944163788450000e+000) (2, -1.23703144911125430000e-001) (3, 7.65139686450641120000e-001) (4, -5.96611755768111430000e+000) (5, 3.76684660592590030000e+000) (6, 4.56109203875254380000e+000) (7, -7.69520687683525000000e-001) (8, -1.97108810499239400000e-002) (0, 3.05780152616603250000e+000) (1, -3.36625146896971780000e+000) (2, -3.90475050512564650000e-001) (3, -1.46393349304809780000e+000) (4, 7.09479646044468120000e+000) (5, -3.21944507966061180000e+000) (6, -7.87763104825066930000e+000) (7, 2.44756173263426380000e+000) (8, -6.06466957631576140000e-002) (9, -2.31487698134484710000e+000) (10, -2.86892040938158340000e+000) (11, -2.13014143065465420000e+000) (12, 4.52772721308814740000e-001) (9, 9.10834178456745210000e+000) (10, 7.33271989799850310000e+000) (11, 4.19503577661039890000e+000) (12, 1.14042077885594550000e+000) (9, -3.15486427179298930000e+000) (10, -5.07727917598748910000e-001) (11, -1.05368773329243710000e-001) (12, 9.48456080646307930000e-001) (9, 7.34615343776420280000e+000) (10, 3.33023084197528220000e+000) (11, 1.58286348476644980000e+000) (12, 1.18060313270114280000e+000) (9, -6.29493052777215480000e+000) (10, -1.73742795410676140000e+000) (11, -4.82058014658928380000e-002) (12, 9.91706360172347260000e-001) 
