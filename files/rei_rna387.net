FANN_FLO_2.1
num_layers=3
learning_rate=0.700000
connection_rate=1.000000
network_type=0
learning_momentum=0.000000
training_algorithm=2
train_error_function=1
train_stop_function=0
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_max_cand_epochs=150
cascade_num_candidate_groups=2
bit_fail_limit=3.49999999999999980000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000000000000020000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=9 11 6 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (9, 3, 5.00000000000000000000e-001) (9, 3, 5.00000000000000000000e-001) (9, 3, 5.00000000000000000000e-001) (9, 3, 5.00000000000000000000e-001) (9, 3, 5.00000000000000000000e-001) (9, 3, 5.00000000000000000000e-001) (9, 3, 5.00000000000000000000e-001) (9, 3, 5.00000000000000000000e-001) (9, 3, 5.00000000000000000000e-001) (9, 3, 5.00000000000000000000e-001) (0, 3, 0.00000000000000000000e+000) (11, 3, 5.00000000000000000000e-001) (11, 3, 5.00000000000000000000e-001) (11, 3, 5.00000000000000000000e-001) (11, 3, 5.00000000000000000000e-001) (11, 3, 5.00000000000000000000e-001) (0, 3, 0.00000000000000000000e+000) 
connections (connected_to_neuron, weight)=(0, 1.50000000000000000000e+003) (1, -6.14499039319063510000e+002) (2, -9.27579739810411470000e+001) (3, 8.96132043294397820000e+002) (4, 1.14018153979240740000e+003) (5, -1.48916142527873310000e+003) (6, 5.38479070239778930000e+002) (7, -5.08672741407482530000e+002) (8, 2.21476184985181850000e+000) (0, -5.67855022547653650000e+000) (1, -1.50000000000000000000e+003) (2, -1.50000000000000000000e+003) (3, 2.52515365163062370000e+002) (4, 6.99614908555887720000e+002) (5, -3.45463785520229240000e+002) (6, 1.04639455041402330000e+002) (7, 7.92585654023044270000e+001) (8, -4.43830506477543630000e+000) (0, 5.60813315981744440000e+001) (1, 6.89255591590930860000e+002) (2, 2.04837728109799830000e+002) (3, -1.50000000000000000000e+003) (4, 7.87818770440115940000e+001) (5, 4.57699315620836610000e+002) (6, 7.60804426365659480000e+001) (7, -1.04148838929620740000e+002) (8, -3.55025333223424080000e+001) (0, 1.79368838263314810000e+002) (1, -2.84256006076671210000e+002) (2, 1.50000000000000000000e+003) (3, 1.10720366403482080000e+003) (4, -7.40625414470901550000e+002) (5, -2.83655144223441250000e+002) (6, -3.76464377656167240000e+002) (7, 6.91439426513529270000e+002) (8, 5.57783356652092350000e+000) (0, 2.09409872225655890000e+001) (1, 3.18017849750817440000e+000) (2, -1.27796056249195490000e+002) (3, 2.44901232081194540000e+002) (4, 4.72795240038785420000e+001) (5, 2.70865430602168330000e+001) (6, -4.21734426695677130000e+001) (7, -1.50000000000000000000e+003) (8, -6.38577108530346130000e+000) (0, 2.74281221294802260000e+000) (1, -3.01272775882519320000e+001) (2, 6.90349364246633570000e+001) (3, 6.48172167668950290000e+001) (4, -3.27860246148646810000e+002) (5, 1.10101435836300280000e+002) (6, 1.41234042842364090000e+001) (7, -1.83081623448026560000e+002) (8, 4.60272119276054340000e+000) (0, 2.95019264661188930000e+000) (1, 7.12947741842656340000e+001) (2, -4.91328547522150460000e+001) (3, -1.59859183265555340000e+002) (4, 1.99376510489586020000e+002) (5, 7.38231188474069720000e+000) (6, 4.35633656994647250000e+001) (7, 4.27367308209051660000e+001) (8, -4.71052822403802200000e+000) (0, 2.09960880483216780000e+001) (1, -8.31409444986474990000e+001) (2, -4.78667000480697350000e+000) (3, -3.97627773889472280000e+001) (4, -1.60676111158776610000e+001) (5, 6.36942426286926850000e+001) (6, -6.04455075191106420000e+001) (7, 4.15053579012644180000e+001) (8, 6.73180886686746230000e+000) (0, 2.43253640662055940000e+002) (1, -4.47675173409506670000e+002) (2, 1.28641888404117420000e+002) (3, -7.44780629606042790000e+002) (4, -5.35925463659395750000e+002) (5, 1.50000000000000000000e+003) (6, -7.62884936669345960000e+002) (7, 3.13135114407558660000e+002) (8, 4.81370686883957790000e+001) (0, -7.21657469220932590000e+000) (1, -1.51038078604328630000e+002) (2, -2.06518140784367490000e+002) (3, 1.52131915046965730000e+002) (4, 2.13035704940288270000e+002) (5, -2.31666995435595600000e+002) (6, -6.81913570227533650000e+001) (7, 1.23199883862066390000e+001) (8, -1.51302599389496000000e+000) (9, -1.05038319736119980000e-001) (10, 8.40280569857416730000e-001) (11, 1.80042618080143960000e-001) (12, -2.05782557443326690000e-002) (13, -3.01223784369030360000e-001) (14, -2.75910554942551230000e-001) (15, -3.75067919310819900000e-001) (16, 8.19368463326703990000e-001) (17, -1.08292232566263550000e+000) (18, -7.60633573877693260000e-001) (19, -2.14797101326786480000e-001) (9, -2.39509475975022870000e-001) (10, -8.24957550042115350000e-001) (11, -1.56674116300582870000e-001) (12, -1.88450955124959020000e-001) (13, -2.53653860313188540000e-001) (14, 1.07255857241619570000e-001) (15, 1.25061048830861670000e-001) (16, -4.41177970566197120000e-001) (17, 6.56588076120550150000e-001) (18, 4.79472479708525910000e-001) (19, -3.60505051252298480000e-001) (9, -1.46368258205011560000e-001) (10, -6.06636730314219940000e-001) (11, -5.60261251123281290000e-001) (12, -7.23783378124612800000e-002) (13, -6.32381278778605060000e-001) (14, 2.22366028198270530000e-001) (15, 2.41367483715063720000e-001) (16, -1.03108346606381680000e+000) (17, 7.58312001800419130000e-001) (18, 4.68745781702265940000e-001) (19, 8.38129208503228810000e-002) (9, 2.97982680223131400000e-001) (10, 8.62515952799545490000e-003) (11, 4.96061285681878290000e-001) (12, -7.87825053851913040000e-001) (13, 4.84617401308103880000e-001) (14, -6.06933941192708380000e-001) (15, -9.49076897400581990000e-001) (16, 2.51145695670184080000e-001) (17, -3.80703787944165410000e-002) (18, -4.78706468898850710000e-001) (19, 7.97420992473534480000e-001) (9, 1.11888335265995830000e-001) (10, -4.47829470241377540000e-001) (11, -3.77287233543492780000e-002) (12, -2.05072828486474430000e-001) (13, -4.40480588474145950000e-001) (14, -7.10249766578096580000e-002) (15, -5.00630529561125260000e-002) (16, 2.43735604585703400000e-002) (17, 1.63836044430177910000e-001) (18, 5.69850762792769250000e-001) (19, -9.56521812333410580000e-002) 
